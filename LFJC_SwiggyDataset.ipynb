{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzVTDxu/LFp/el/PWtv5Uk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LydiaMarina/Lead_Scoring_CaseStudy/blob/main/LFJC_SwiggyDataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-rb4dMFjDnP"
      },
      "outputs": [],
      "source": [
        "# Install Python packages using pip.\n",
        "\n",
        "# The \"!pip\" command allows you to run shell commands in Jupyter Notebook or Colab cells.\n",
        "# It is used here to install Python packages.\n",
        "# The \"-q\" flag stands for \"quiet,\" which means it will suppress output during installation.\n",
        "# \"feature_engine,\" \"autoviz,\" and \"dataprep\" are the packages being installed.\n",
        "# The \"2>/dev/null\" part redirects any error messages (stderr) to the null device, effectively silencing them.\n",
        "# This is often used when you want to hide installation messages.\n",
        "!pip install -q feature_engine autoviz>=0.1.803 dataprep 2>/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np  # Import NumPy for handling numerical operations\n",
        "import pandas as pd  # Import Pandas for data manipulation and analysis\n",
        "import warnings  # Import Warnings to suppress unnecessary warnings\n",
        "\n",
        "# Suppress warning messages\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Import AutoViz from the autoviz library for automated visualization of data\n",
        "from autoviz import AutoViz_Class\n",
        "\n",
        "# Import load_dataset and create_report from the dataprep library for data loading and EDA\n",
        "from dataprep.datasets import load_dataset\n",
        "from dataprep.eda import create_report\n",
        "\n",
        "# Import SHAP for interpreting model predictions\n",
        "import shap\n",
        "\n",
        "# Import matplotlib for data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import CatBoostRegressor for building a regression model\n",
        "from catboost import Pool, CatBoostRegressor\n",
        "\n",
        "# Import mean_squared_error for evaluating model performance\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Import train_test_split for splitting the data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import RareLabelEncoder from feature_engine.encoding for encoding categorical features\n",
        "from feature_engine.encoding import RareLabelEncoder\n",
        "\n",
        "# Import CountVectorizer from sklearn.feature_extraction.text for text feature extraction\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Import ast and re for working with text and regular expressions\n",
        "import ast\n",
        "import re\n",
        "\n",
        "# Set Pandas options to display a maximum of 1000 rows\n",
        "pd.set_option('display.max_rows', 1000)"
      ],
      "metadata": {
        "id": "zd9_NKl2jYfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "df = pd.read_csv('/kaggle/input/swiggy-restaurants-dataset/swiggy_file.csv')  # Reads the dataset from a CSV file into a Pandas DataFrame\n",
        "item0 = df.shape[0]  # Stores the initial number of rows in the DataFrame\n",
        "df = df.drop_duplicates()  # Removes duplicate rows from the DataFrame\n",
        "item1 = df.shape[0]  # Stores the number of rows after removing duplicates\n",
        "print(f\"There are {item0-item1} duplicates found in the dataset\")  # Prints the number of duplicates that were removed\n",
        "\n",
        "def extract_rating(x):\n",
        "    try:\n",
        "        return float(x)\n",
        "    except:\n",
        "        return None\n",
        "df['Rating'] = df['Rating'].apply(extract_rating)\n",
        "# Select only records with positive ratings\n",
        "df = df[df['Rating']>0]\n",
        "\n",
        "df['Area'] = df['Area'].apply(lambda x: str(x).split('\\n')[0])\n",
        "\n",
        "df['Location'] = df['Area'] + ', ' + df['Location']\n",
        "\n",
        "# Select only specific columns of interest\n",
        "selected_cols = ['Rating', 'Cuisine', 'Average Price', 'Pure Veg', 'Location']\n",
        "df = df[selected_cols]\n",
        "\n",
        "print(df.shape)  # Prints the dimensions (rows and columns) of the filtered DataFrame\n",
        "df.sample(5).T  # Displays a random sample of 5 rows transposed for better visibility"
      ],
      "metadata": {
        "id": "bRosl3U2jkhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "zON5-Mw_jsH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.nunique()"
      ],
      "metadata": {
        "id": "Z4nyVtZZjuo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "gSgFBsD2jyK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# An update taken from the nice work https://www.kaggle.com/code/anshtanwar/auto-eda-missing-migrants-interactive-charts\n",
        "# made by @anshtanwar\n",
        "\n",
        "# Import the AutoViz_Class\n",
        "# This class is used for automated exploratory data analysis and visualization.\n",
        "AV = AutoViz_Class()\n",
        "\n",
        "# Initialize variables\n",
        "filename = \"\"  # Specify the filename of the dataset (empty in this case)\n",
        "target_variable = 'Rating'  # Specify the target variable for analysis\n",
        "custom_plot_dir = \"custom_plot_directory\"  # Specify the directory to save custom plots\n",
        "\n",
        "# Perform automated EDA using the AutoViz library\n",
        "# The following parameters are used:\n",
        "# - filename: Empty in this case as the data is provided directly as 'df'\n",
        "# - sep: Delimiter used in the data (comma in this case)\n",
        "# - depVar: Target variable for analysis ('rating' in this case)\n",
        "# - dfte: DataFrame to be analyzed ('df' is assumed to be defined earlier)\n",
        "# - header: Indicates that the first row contains column names (0 for True)\n",
        "# - verbose: Verbosity level (1 for verbose output)\n",
        "# - lowess: Smoothing using Lowess algorithm (False for no smoothing)\n",
        "# - chart_format: Format in which charts will be generated (HTML format in this case)\n",
        "# - max_rows_analyzed: Maximum number of rows to analyze (up to 10,000 rows)\n",
        "# - max_cols_analyzed: Maximum number of columns to analyze (up to 50 columns)\n",
        "# - save_plot_dir: Directory to save the generated plots ('custom_plot_directory' in this case)\n",
        "try:\n",
        "    dft = AV.AutoViz(\n",
        "        filename,\n",
        "        sep=\",\",\n",
        "        depVar=target_variable,\n",
        "        dfte=df,\n",
        "        header=0,\n",
        "        verbose=1,\n",
        "        lowess=False,\n",
        "        chart_format=\"html\",\n",
        "        max_rows_analyzed=min([df.shape[0], 10**4]),\n",
        "        max_cols_analyzed=min([df.shape[1], 50]),\n",
        "        save_plot_dir=custom_plot_dir\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"Exception: {e}\")\n"
      ],
      "metadata": {
        "id": "c6_yJ9aXj9Rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary library for displaying HTML content\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "# Import the pathlib library to work with file paths\n",
        "from pathlib import Path\n",
        "\n",
        "# Initialize an empty list to store file names\n",
        "file_names = []\n",
        "\n",
        "# Use pathlib to iterate through HTML files in a specific directory\n",
        "for file in Path(f'/kaggle/working/{custom_plot_dir}/{target_variable}/').glob('*.html'):\n",
        "\n",
        "    # Extract the filename from the full path and add it to the list\n",
        "    filename = str(file).split('/')[-1]\n",
        "    file_names.append(filename)\n",
        "\n",
        "# Iterate through the list of file names and display each HTML file\n",
        "for file_name in file_names:\n",
        "\n",
        "    # Construct the full file path for each HTML file\n",
        "    file_path = f'/kaggle/working/{custom_plot_dir}/{target_variable}/{file_name}'\n",
        "\n",
        "    # Open the HTML file for reading\n",
        "    with open(file_path, 'r') as file:\n",
        "\n",
        "        # Read the content of the HTML file\n",
        "        html_content = file.read()\n",
        "\n",
        "        # Display the HTML content using IPython\n",
        "        display(HTML(html_content))"
      ],
      "metadata": {
        "id": "9H8E-k5Uj97D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_report(df.sample(10**3))"
      ],
      "metadata": {
        "id": "9pPe4ZU-kEWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns #Data Transformation"
      ],
      "metadata": {
        "id": "UAmNz5ttkHHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(5).T"
      ],
      "metadata": {
        "id": "0wCLVaMQkv6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Select the main label.\n",
        "main_label = 'Rating'\n",
        "\n",
        "# vectorize columns\n",
        "def vectorize_column(df, col_name, min_df=20):\n",
        "    ll = df[col_name].fillna('none').str.split(', ').to_list()\n",
        "    ll = [[j.rstrip(', ').strip(' ').replace('(', '_').replace(')', '_').replace('/', '_').replace('.', '_').replace('+', '_').replace(',', '_').replace('\\'', '_').replace(' ', '_').replace('.', '_').replace('&', '_').replace('-', '_').replace('!', '_') for j in i] for i in ll]\n",
        "    ll1 = []\n",
        "    for item in ll:\n",
        "        if item != ['none']:\n",
        "            ttt = ' '.join(item)\n",
        "        else:\n",
        "            ttt = 'none'\n",
        "        ll1.append(ttt)\n",
        "    vectorizer = CountVectorizer(max_features=120, min_df=min_df, lowercase=False)\n",
        "    vectorizer.fit(ll1)\n",
        "    voc = vectorizer.vocabulary_\n",
        "    voc_inv = {v: col_name+'_'+k for k, v in voc.items()}\n",
        "    vector = vectorizer.transform(ll1)\n",
        "    tt = pd.DataFrame(vector.toarray())\n",
        "    tt = tt.rename(columns=voc_inv)\n",
        "    df = pd.concat([df.reset_index(drop=True),tt.reset_index(drop=True)], axis=1).drop([col_name], axis=1)\n",
        "    return df\n",
        "for col in ['Cuisine']:\n",
        "    df = vectorize_column(df, col_name=col, min_df=20)\n",
        "\n",
        "\n",
        "\n",
        "# Set up a rare label encoder for selected columns.\n",
        "for col in ['Average Price', 'Pure Veg', 'Location']:\n",
        "    df[col] = df[col].fillna('None')\n",
        "    encoder = RareLabelEncoder(n_categories=1, max_n_categories=120, replace_with='Other', tol=20.0 / df.shape[0])\n",
        "    df[col] = encoder.fit_transform(df[[col]])\n",
        "\n",
        "print(df.shape)  # Print the shape of the resulting DataFrame.\n",
        "df.sample(10).T  # Display a sample of 10 rows, transposed for easier readability."
      ],
      "metadata": {
        "id": "FGRQMhXSkyVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize data\n",
        "# Extract the values of the 'main_label' column and reshape it into a 1D array as 'y'\n",
        "y = df[main_label].values.reshape(-1,)\n",
        "\n",
        "# Create the feature matrix 'X' by dropping the 'main_label' column from the DataFrame 'df'\n",
        "X = df.drop([main_label], axis=1)\n",
        "\n",
        "# Identify categorical columns in the DataFrame 'df'\n",
        "# These columns contain non-numeric data\n",
        "cat_cols = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Create a list of indices for categorical columns in the feature matrix 'X'\n",
        "cat_cols_idx = [list(X.columns).index(c) for c in cat_cols]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "# - 'X_train' and 'y_train' will contain the training features and labels, respectively\n",
        "# - 'X_test' and 'y_test' will contain the testing features and labels, respectively\n",
        "# The split is done with a 50% test size, a random seed of 0, and stratification based on the 'Price' column\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0, stratify=df[['Location']])\n",
        "\n",
        "# Print the dimensions of the training and testing sets\n",
        "# This provides insight into the sizes of the datasets\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "Vz7ge-S5k1ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Initialize the training and testing data pools using CatBoost's Pool class\n",
        "train_pool = Pool(X_train,\n",
        "                  y_train,\n",
        "                  cat_features=cat_cols_idx)  # Create a training data pool with categorical features\n",
        "test_pool = Pool(X_test,\n",
        "                 y_test,\n",
        "                 cat_features=cat_cols_idx)  # Create a testing data pool with categorical features\n",
        "\n",
        "# Specify the training parameters for the CatBoostRegressor model\n",
        "model = CatBoostRegressor(iterations=1000,    # Number of boosting iterations\n",
        "                          depth=7,           # Maximum depth of trees in the ensemble\n",
        "                          verbose=0,         # Set verbosity level to 0 (no output during training)\n",
        "                          learning_rate=0.08,  # Learning rate for gradient boosting\n",
        "                          early_stopping_rounds=10, # Early stopping rounds\n",
        "                          loss_function='RMSE')  # Loss function to optimize (Root Mean Squared Error)\n",
        "\n",
        "# Train the CatBoostRegressor model on the training data\n",
        "model.fit(train_pool, eval_set=test_pool)\n",
        "\n",
        "# Make predictions using the trained model on both the training and testing data\n",
        "y_train_pred = model.predict(train_pool)  # Predictions on the training data\n",
        "y_test_pred = model.predict(test_pool)    # Predictions on the testing data\n",
        "\n",
        "# Calculate and print the Root Mean Squared Error (RMSE) scores for training and testing data\n",
        "rmse_train = mean_squared_error(y_train, y_train_pred, squared=False)  # RMSE for training data\n",
        "rmse_test = mean_squared_error(y_test, y_test_pred, squared=False)     # RMSE for testing data\n",
        "\n",
        "# Print the rounded RMSE scores\n",
        "print(f\"RMSE score for train {round(rmse_train, 3)} points, and for test {round(rmse_test, 3)} points\")"
      ],
      "metadata": {
        "id": "TmUPcEpvk8it"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the baseline RMSE (Root Mean Squared Error) scores for the training and test datasets.\n",
        "\n",
        "# For the training dataset:\n",
        "\n",
        "# Calculate the RMSE by comparing the actual target values (y_train) with the predicted values,\n",
        "# where the predicted values are the mean of the training target values repeated for each data sample.\n",
        "rmse_bs_train = mean_squared_error(y_train, [np.mean(y_train)]*len(y_train), squared=False)\n",
        "\n",
        "# For the test dataset:\n",
        "\n",
        "# Calculate the RMSE by comparing the actual target values (y_test) with the predicted values,\n",
        "# where the predicted values are the mean of the training target values repeated for each test data sample.\n",
        "rmse_bs_test = mean_squared_error(y_test, [np.mean(y_train)]*len(y_test), squared=False)\n",
        "\n",
        "# Print the rounded baseline RMSE scores for both the training and test datasets.\n",
        "print(f\"RMSE baseline score for train {round(rmse_bs_train, 3)} points, and for test {round(rmse_bs_test, 3)} points\")"
      ],
      "metadata": {
        "id": "1vaXRrI_lAZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cgM4mHjElD7-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}